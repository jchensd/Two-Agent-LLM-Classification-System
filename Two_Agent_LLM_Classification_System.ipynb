{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNLwiP0wiPfKl0GiNPmwGIR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Two-Agent LLM Classification for Societal Text Data\n",
        "\n",
        "This notebook implements a two-agent LLM text classification pipeline where a **high-level controller agent** adaptively designs and refines the classification prompt and evaluation plan, and a **worker agent** labels individual texts row-by-row based on the classification prompt. Both agents are powered by the OpenAI API, and this notebook allows users to choose which OpenAI models to use for the controller and the worker.\n",
        "\n",
        "The pipeline is demonstrated on:\n",
        "- a Chinese news dataset with labels Challenge vs Source (stance toward foreign news outlets)\n",
        "- the **EZ-STANCE** dataset for English tweet stance detection(favor, against, none)\n",
        "\n"
      ],
      "metadata": {
        "id": "8x1taDN86uGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Configuration"
      ],
      "metadata": {
        "id": "Tk4qWBU4GaP1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cavMemV-TEHX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import requests\n",
        "import time\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
        "import io\n",
        "import contextlib\n",
        "import traceback\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data path configuration\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path = '/content/drive/MyDrive/Agentic Labeling System/'"
      ],
      "metadata": {
        "id": "YygGyTRUTrPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#safely entering parameters to use Cloudflare proxy for calling LLM API\n",
        "\n",
        "if not os.environ.get(\"CF_WORKER_URL\"):\n",
        "    os.environ[\"CF_WORKER_URL\"] = getpass.getpass(\"Enter Cloudflare worker URL (input hidden): \")\n",
        "\n",
        "if not os.environ.get(\"CF_PROXY_TOKEN\"):\n",
        "    os.environ[\"CF_PROXY_TOKEN\"] = getpass.getpass(\"Enter Cloudflare proxy token (input hidden): \")\n",
        "\n",
        "WORKER_URL = os.environ[\"CF_WORKER_URL\"]\n",
        "PROXY_TOKEN = os.environ[\"CF_PROXY_TOKEN\"]"
      ],
      "metadata": {
        "id": "Spy6lQ5LUAcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Two-Agent Labeling Pipeline Implementation"
      ],
      "metadata": {
        "id": "DZT6UimbIMYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###---Helper Functions (API Call)---###\n",
        "\n",
        "#call OpenAI chat completions through the Cloudflare worker\n",
        "def openai_chat_via_cf(\n",
        "    model,            #the GPT model\n",
        "    messages,         #input messages\n",
        "    temperature=0,\n",
        "    max_retries=3,    #the maximum number of retries when encountering transident errors\n",
        "    retry_backoff_seconds=5, #base delay time for backoff between retries\n",
        "    **extra_params,   #additional parameters to send long in the JSON\n",
        "):\n",
        "    #base payload for OpenAI Chat API\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages,\n",
        "    }\n",
        "\n",
        "    #for GPT 5 family, temperature and token limit parameters are not allowed\n",
        "    gpt5_family = {\"gpt-5\", \"gpt-5-mini\", \"gpt-5-nano\"}\n",
        "    if model not in gpt5_family:\n",
        "        payload[\"temperature\"] = temperature\n",
        "        payload.update(extra_params)\n",
        "    else:\n",
        "        allowed_extra_params = {}\n",
        "        for k, v in extra_params.items():\n",
        "            if k in {\"max_tokens\", \"max_output_tokens\", \"max_completion_tokens\"}:\n",
        "                continue\n",
        "            if k == \"temperature\":\n",
        "                continue\n",
        "            allowed_extra_params[k] = v\n",
        "        payload.update(allowed_extra_params)\n",
        "\n",
        "    #retry loop\n",
        "    last_error = None #the last error seen\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            #for each attempt, send an HTTP POST to WORKER_URL\n",
        "            resp = requests.post(\n",
        "                WORKER_URL,\n",
        "                headers={\n",
        "                    \"x-proxy-token\": PROXY_TOKEN,\n",
        "                    \"Content-Type\": \"application/json\",\n",
        "                },\n",
        "                json=payload,\n",
        "                timeout=120,\n",
        "            )\n",
        "\n",
        "            #handling HTTP errors\n",
        "            if resp.status_code in (429,) or 500 <= resp.status_code < 600:\n",
        "              #retry only on transient HTTP codes (429 or 5xx)\n",
        "                print(f\"[openai_chat_via_cf] Transient error (status {resp.status_code}) on attempt {attempt}/{max_retries}\")\n",
        "                try:\n",
        "                    print(\"  Payload of error:\", resp.json())\n",
        "                except Exception:\n",
        "                    print(\"  Raw error text:\", resp.text)\n",
        "                last_error = requests.HTTPError(f\"Status {resp.status_code}\")\n",
        "                if attempt < max_retries:\n",
        "                    sleep_for = retry_backoff_seconds * attempt\n",
        "                    print(f\"  Sleeping {sleep_for} seconds before retrying...\")\n",
        "                    time.sleep(sleep_for)\n",
        "                    continue\n",
        "                else:\n",
        "                    resp.raise_for_status()\n",
        "\n",
        "            #for other HTTP codes, raise if error\n",
        "            resp.raise_for_status()\n",
        "            return resp.json()\n",
        "\n",
        "        #for requests-level issue, log the exception\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"[openai_chat_via_cf] Request exception on attempt {attempt}/{max_retries}: {e}\")\n",
        "            #store the expection\n",
        "            last_error = e\n",
        "            if attempt < max_retries:\n",
        "                sleep_for = retry_backoff_seconds * attempt\n",
        "                print(f\"  Sleeping {sleep_for} seconds before retrying...\")\n",
        "                time.sleep(sleep_for)\n",
        "                continue\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    #defensive fallback: in case exit the loop without returning or raising\n",
        "    if last_error is not None:\n",
        "        #reraise the last captured error\n",
        "        raise last_error\n"
      ],
      "metadata": {
        "id": "DxZALbmLUO3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###---Helper Functions (Basic Utilities)---###\n",
        "\n",
        "#Given a dataframe with ground-truth labels and LLM predicted labels,\n",
        "#build a clean metrics dictionary that can log or feed back to the agent.\n",
        "def build_metrics_summary(df,         #dataframe\n",
        "                          label_col,  #the column of ground-truth labels\n",
        "                          pred_col,   #the column of LLM predicted labels\n",
        "                          labels_list #the list of candidate labels\n",
        "                          ):\n",
        "\n",
        "    y_true = df[label_col].values\n",
        "    y_pred = df[pred_col].values\n",
        "\n",
        "    #compute overall accuracy across all rows\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    #compute per-label precision, recall, F1 score, number of true examples for each label\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_true, y_pred, labels=labels_list,\n",
        "        zero_division=0  #avoid crashes when a label has no predicted positives\n",
        "    )\n",
        "\n",
        "    #build per-label dictionary\n",
        "    per_label_dic = {}\n",
        "    for lab, p, r, f, s in zip(labels_list, precision, recall, f1, support):\n",
        "        per_label_dic[lab] = {\n",
        "            \"precision\": float(p),\n",
        "            \"recall\": float(r),\n",
        "            \"f1\": float(f),\n",
        "            \"support\": int(s),\n",
        "        }\n",
        "    #average of per-label F1 scores (unweighted)\n",
        "    macro_f1 = float(sum(f1) / len(f1)) if len(f1) > 0 else 0\n",
        "\n",
        "    summary = {\n",
        "        \"overall\": {\n",
        "            \"num_eval_rows\": int(len(df)),\n",
        "            \"accuracy\": float(acc),\n",
        "            \"macro_f1\": macro_f1,\n",
        "        },\n",
        "        \"per_label\": per_label_dic,\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "#if use more than the text column as input for LLM clasification,\n",
        "#use the user-customized function to turn the columns into messages\n",
        "def model_input_from_row(row,             #one row of the dataframe\n",
        "                         TEXT_COL,        #main text column\n",
        "                         row_to_text_fn): #an optional function provided to build a custom string from the row\n",
        "\n",
        "    if row_to_text_fn is not None:\n",
        "        return str(row_to_text_fn(row))\n",
        "    return str(row[TEXT_COL])"
      ],
      "metadata": {
        "id": "9m1FJdmzl9eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###---Helper Functions (Tools the agent can request)---###\n",
        "\n",
        "#give the agent global information about the dataset\n",
        "def tool_get_summary(df_train, labels, label_counts):\n",
        "    return {\n",
        "        \"num_rows\": int(len(df_train)), #size of the training set\n",
        "        \"labels\": labels,               #list of label names\n",
        "        \"label_counts\": label_counts,   #the number of rows for each label\n",
        "    }\n",
        "\n",
        "#give a small preview of the first n rows for the agent\n",
        "def tool_get_head(df_train, TEXT_COL, LABEL_COL, n=10, row_to_text_fn=None):\n",
        "    n = min(int(n), len(df_train))   #avoid asking for more rows than exist\n",
        "    subset = df_train.head(n).copy() #take the first n rows\n",
        "\n",
        "    #convert the texts and labels into a dictionary\n",
        "    records = subset[[TEXT_COL, LABEL_COL]].to_dict(orient=\"records\")\n",
        "\n",
        "    #add combined information if extra columns are used as LLM classification input\n",
        "    if row_to_text_fn is not None:\n",
        "        for i, (_, row) in enumerate(subset.iterrows()):\n",
        "            records[i][\"model_input\"] = str(row_to_text_fn(row))\n",
        "\n",
        "    return records\n",
        "\n",
        "#give N random examples for each label\n",
        "def tool_sample_per_label(df_train, TEXT_COL, LABEL_COL, per_label_n, random_state, row_to_text_fn=None):\n",
        "    result = {}\n",
        "    for lab, n in per_label_n.items():\n",
        "        n = int(n)\n",
        "        if n <= 0:\n",
        "            continue\n",
        "        subset = df_train[df_train[LABEL_COL] == lab]\n",
        "        if len(subset) == 0:\n",
        "            continue\n",
        "        sample_n = min(n, len(subset))\n",
        "        sampled = subset.sample(n=sample_n, random_state=random_state).copy()\n",
        "\n",
        "        records = sampled[[TEXT_COL, LABEL_COL]].to_dict(orient=\"records\")\n",
        "\n",
        "        if row_to_text_fn is not None:\n",
        "            for i, (_, row) in enumerate(sampled.iterrows()):\n",
        "                records[i][\"model_input\"] = str(row_to_text_fn(row))\n",
        "\n",
        "        result[lab] = records\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "SB_hxyQKp1GR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###---Helper Functions (Safe sandbox for executing agent-provided Python code)---###\n",
        "#Important Note: This function is intended for experimental use only.\n",
        " #- While it restricts access to unsafe operations and limits available built-in functions,\n",
        " # it is not guaranteed to be fully secure.\n",
        " # I'm using it here with caution.\n",
        "\n",
        "#define the set of built-in functions that are safe to execute\n",
        "SAFE_BUILTINS = {\n",
        "    \"print\": print,\n",
        "    \"len\": len,\n",
        "    \"range\": range,\n",
        "    \"min\": min,\n",
        "    \"max\": max,\n",
        "    \"sum\": sum,\n",
        "    \"sorted\": sorted,\n",
        "    \"enumerate\": enumerate,\n",
        "    \"list\": list,\n",
        "    \"dict\": dict,\n",
        "    \"set\": set,\n",
        "    \"tuple\": tuple,\n",
        "    \"any\": any,\n",
        "    \"all\": all,\n",
        "    \"abs\": abs,\n",
        "    \"round\": round,\n",
        "    \"str\": str,\n",
        "    \"int\": int,\n",
        "    \"float\": float,\n",
        "}\n",
        "\n",
        "#define the set of functions that are not allowed to execute\n",
        "DISALLOWED_TOKENS = [\n",
        "    \"import \",\n",
        "    \"import\\t\",\n",
        "    \"import\\n\",\n",
        "    \"from \",\n",
        "    \"open(\",\n",
        "    \"os.\",\n",
        "    \"sys.\",\n",
        "    \"subprocess\",\n",
        "    \"socket\",\n",
        "    \"requests\",\n",
        "    \"http.\",\n",
        "    \"urllib\",\n",
        "]\n",
        "\n",
        "#sandbox executor for LLM agent-written code\n",
        "#It executes agent-provided Python code in a restricted environment with:\n",
        " #- no imports allowed\n",
        " #- no access to os, sys, requests, network, etc.\n",
        " #- only safe builtins\n",
        " #- only three objects exposed (df_train, TEXT_COL, LABEL_COL).\n",
        "def run_python_view_sandbox(\n",
        "    code_str,\n",
        "    df_train,\n",
        "    TEXT_COL,\n",
        "    LABEL_COL,\n",
        "    max_output_chars=4000,\n",
        "):\n",
        "    #Before executing anything,\n",
        "    #if any of the forbidden functions appears inside the code string,\n",
        "    #immediately reject it.\n",
        "    for token in DISALLOWED_TOKENS:\n",
        "        if token in code_str:\n",
        "            return {\n",
        "                \"status\": \"error\",\n",
        "                \"error\": f\"Disallowed token detected in code: {token.strip()}\",\n",
        "                \"stdout\": \"\",\n",
        "                \"stderr\": \"\",\n",
        "            }\n",
        "\n",
        "    #prepare restricted global environment\n",
        "    env = {\n",
        "        \"__builtins__\": SAFE_BUILTINS,\n",
        "        \"df_train\": df_train,\n",
        "        \"TEXT_COL\": TEXT_COL,\n",
        "        \"LABEL_COL\": LABEL_COL,\n",
        "    }\n",
        "\n",
        "    #in-memory string buffers\n",
        "    stdout_buf = io.StringIO() #capture anything that the code prints to standard output\n",
        "    stderr_buf = io.StringIO() #capture errors or anything printed to standard error\n",
        "\n",
        "    try:\n",
        "        #temporarily redirects Python’s global sys.stdout and sys.stderr to StringIO buffers for the duration of the with block\n",
        "        with contextlib.redirect_stdout(stdout_buf), contextlib.redirect_stderr(stderr_buf):\n",
        "            exec(code_str, env, {}) #runs the agent-written Python code string\n",
        "        status = \"ok\"\n",
        "    except Exception:\n",
        "        #return the error details as part of the function result\n",
        "        traceback.print_exc(file=stderr_buf) #capture full traceback into stderr\n",
        "        status = \"error\"\n",
        "\n",
        "    #After execution,\n",
        "    #return the entire captured content of stdout as a string\n",
        "    stdout_val = stdout_buf.getvalue()\n",
        "    #return the error output / traceback as a string\n",
        "    stderr_val = stderr_buf.getvalue()\n",
        "\n",
        "    #truncate outputs to avoid excessively long outputs\n",
        "    #(keeps string size manageable for logs and for feeding back to the agent)\n",
        "    if len(stdout_val) > max_output_chars:\n",
        "        stdout_val = stdout_val[:max_output_chars] + \"\\n...[stdout truncated]\"\n",
        "\n",
        "    if len(stderr_val) > max_output_chars:\n",
        "        stderr_val = stderr_val[:max_output_chars] + \"\\n...[stderr truncated]\"\n",
        "\n",
        "    #return a dictionary that summarizes the execution result\n",
        "    return {\n",
        "        \"status\": status,\n",
        "        \"stdout\": stdout_val,\n",
        "        \"stderr\": stderr_val,\n",
        "    }"
      ],
      "metadata": {
        "id": "wrrP2D1o-XFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###---Helper Function (Agent decides how to look at data)---###\n",
        "\n",
        "def agent_choose_data_view(\n",
        "    user_description_for_agent, #user message\n",
        "    TEXT_COL,                   #the column of main texts\n",
        "    LABEL_COL,                  #the column of ground-truth labels\n",
        "    labels,                     #list of candidate labels\n",
        "    label_counts,               #mapping from each label to its count\n",
        "    MODEL_AGENT,                #LLM model used as the agent\n",
        "):\n",
        "    tools_description = f\"\"\"\n",
        "You have access to the following tools over the labeled dataset.\n",
        "Each row has at least two columns:\n",
        "- '{TEXT_COL}': the main text field to be classified.\n",
        "- '{LABEL_COL}': the human-provided (gold standard) label for that row.\n",
        "\n",
        "Available tools:\n",
        "\n",
        "1. get_summary\n",
        "   - Description: returns a JSON summary with:\n",
        "       - num_rows (int)\n",
        "       - labels (list of label names)\n",
        "       - label_counts (mapping label -> count)\n",
        "   - Params: {{ }}\n",
        "\n",
        "2. get_head\n",
        "   - Description: returns the first N rows as a list of objects with keys '{TEXT_COL}' and '{LABEL_COL}'.\n",
        "   - Params: {{\"n\": <int, number of rows, e.g. 10 or 20>}}\n",
        "\n",
        "3. sample_per_label\n",
        "   - Description: returns, for each label, a sample of up to N[label] rows, as an object:\n",
        "       {{\n",
        "         \"<label_name>\": [{{\"{TEXT_COL}\": ..., \"{LABEL_COL}\": ...}}, ...],\n",
        "         ...\n",
        "       }}\n",
        "   - Params: {{\"per_label_n\": {{\"<label_name>\": <int>, ... }}}}\n",
        "     For example: {{\"per_label_n\": {{\"<label_A>\": 5, \"<label_B>\": 10}}}}\n",
        "\n",
        "4. python_view\n",
        "   - Description: executes a SHORT Python snippet to inspect df_train.\n",
        "     Objects you can use:\n",
        "       - df_train: a pandas DataFrame with all training rows.\n",
        "       - TEXT_COL: the name of the text column (string).\n",
        "       - LABEL_COL: the name of the label column (string).\n",
        "     The environment is RESTRICTED:\n",
        "       - No network access.\n",
        "       - No imports (import/from are blocked).\n",
        "       - No os/sys/requests/etc.\n",
        "       - Only safe builtins and pandas methods on df_train.\n",
        "   - Params: {{\"code\": \"<your Python code as a single string>\"}}\n",
        "\"\"\"\n",
        "\n",
        "    system_msg = (\n",
        "        \"You are an expert in qualitative content analysis and prompt engineering. \"\n",
        "        \"You will first decide which view of the labeled data to inspect, \"\n",
        "        \"then later design a classification prompt and validation plan.\"\n",
        "    )\n",
        "\n",
        "    user_msg = f\"\"\"\n",
        "The user describes the labeled dataset as:\n",
        "\n",
        "{user_description_for_agent}\n",
        "\n",
        "The label column is '{LABEL_COL}', and the labels currently present in the data are:\n",
        "{labels}\n",
        "\n",
        "Approximate label counts:\n",
        "{label_counts}\n",
        "\n",
        "You may request ONE tool call to inspect the labeled data before designing a prompt.\n",
        "{tools_description}\n",
        "\n",
        "Return ONLY a JSON object of the form:\n",
        "\n",
        "{{\n",
        "  \"tool\": \"get_summary\" | \"get_head\" | \"sample_per_label\" | \"python_view\",\n",
        "  \"params\": {{ ... }}\n",
        "}}\n",
        "\n",
        "For python_view, put your code in params.code, for example:\n",
        "{{\n",
        "  \"tool\": \"python_view\",\n",
        "  \"params\": {{\n",
        "    \"code\": \"print('Shape:', df_train.shape)\"\n",
        "  }}\n",
        "}}\n",
        "\n",
        "Be mindful of token limits: do not request extremely large samples.\n",
        "Prefer a modest sample size (for example, 10–30 rows or small per-label samples) for initial inspection.\n",
        "Keep python_view code SHORT and focused on inspecting df_train.\n",
        "\"\"\"\n",
        "\n",
        "    resp = openai_chat_via_cf(\n",
        "        model=MODEL_AGENT,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_msg},\n",
        "            {\"role\": \"user\", \"content\": user_msg},\n",
        "        ],\n",
        "        temperature=TEMPERATURE_AGENT,\n",
        "    )\n",
        "\n",
        "    content = resp[\"choices\"][0][\"message\"][\"content\"]\n",
        "    print(\"=== Agent data-view request (raw) ===\")\n",
        "    print(content)\n",
        "    cmd = json.loads(content)\n",
        "    return cmd"
      ],
      "metadata": {
        "id": "xvvzXwnFqTWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###---Helper Function (Execute the data viewing code the agent requests)---###\n",
        "\n",
        "def execute_data_view_command(\n",
        "    df_train,\n",
        "    TEXT_COL,\n",
        "    LABEL_COL,\n",
        "    labels,\n",
        "    label_counts,\n",
        "    cmd,           #the parsed command from the agent\n",
        "    RANDOM_SEED,\n",
        "    row_to_text_fn=None,\n",
        "):\n",
        "    tool = cmd.get(\"tool\")\n",
        "    params = cmd.get(\"params\", {}) or {}\n",
        "\n",
        "    if tool == \"get_summary\":\n",
        "        result = tool_get_summary(df_train, labels, label_counts)\n",
        "\n",
        "    elif tool == \"get_head\":\n",
        "        n = params.get(\"n\", 10)\n",
        "        result = tool_get_head(\n",
        "            df_train,\n",
        "            TEXT_COL,\n",
        "            LABEL_COL,\n",
        "            n=n,\n",
        "            row_to_text_fn=row_to_text_fn,\n",
        "        )\n",
        "\n",
        "    elif tool == \"sample_per_label\":\n",
        "        per_label_n = params.get(\"per_label_n\", {})\n",
        "        result = tool_sample_per_label(\n",
        "            df_train,\n",
        "            TEXT_COL,\n",
        "            LABEL_COL,\n",
        "            per_label_n,\n",
        "            random_state=RANDOM_SEED,\n",
        "            row_to_text_fn=row_to_text_fn,\n",
        "        )\n",
        "\n",
        "    elif tool == \"python_view\":\n",
        "        code_str = params.get(\"code\", \"\")\n",
        "        #check whether code_str is actually a string\n",
        "        if not isinstance(code_str, str) or not code_str.strip():\n",
        "            raise ValueError(\"python_view tool requires a non-empty string in params['code'].\")\n",
        "        print(\"\\n=== Agent-proposed python_view code ===\")\n",
        "        print(code_str)\n",
        "        result = run_python_view_sandbox(\n",
        "            code_str=code_str,\n",
        "            df_train=df_train,\n",
        "            TEXT_COL=TEXT_COL,\n",
        "            LABEL_COL=LABEL_COL,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown tool: {tool}\")\n",
        "\n",
        "    return tool, params, result"
      ],
      "metadata": {
        "id": "TTV3bISO18ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###---Helper Function (Agent builds initial prompt and validation plan)---###\n",
        "\n",
        "def agent_build_prompt_and_plan(\n",
        "    user_description_for_agent,\n",
        "    TEXT_COL,\n",
        "    LABEL_COL,\n",
        "    labels,\n",
        "    label_counts,\n",
        "    tool_name,\n",
        "    tool_params,\n",
        "    tool_result,\n",
        "    total_examples,\n",
        "    MODEL_AGENT,\n",
        "    has_row_to_text_fn, #boolean flag indicating whether a custom row_to_text_fn is used to construct input text\n",
        "):\n",
        "\n",
        "    #convert results from data viewing into a JSON string\n",
        "    tool_result_str = json.dumps(tool_result, ensure_ascii=False)\n",
        "\n",
        "    #multi-line string with label frequencies\n",
        "    #letting the agent know how balanced/imbalanced the labels are\n",
        "    label_counts_text = \"\\n\".join(\n",
        "        f\"- {lab}: {count} rows\" for lab, count in label_counts.items()\n",
        "    )\n",
        "\n",
        "    if not has_row_to_text_fn:\n",
        "        classifier_input_desc = (\n",
        "            f\"The classifier model will receive as input the raw value from the '{TEXT_COL}' column.\"\n",
        "        )\n",
        "        input_requirement_sentence = (\n",
        "            f\"- State that the input will be a single text value taken from the '{TEXT_COL}' column.\"\n",
        "        )\n",
        "    else:\n",
        "        classifier_input_desc = (\n",
        "            \"The classifier model will receive as input a single combined text string \"\n",
        "            \"constructed from multiple columns in each row \"\n",
        "            \"The main text column is \"\n",
        "            f\"'{TEXT_COL}'.\"\n",
        "        )\n",
        "        input_requirement_sentence = (\n",
        "            \"- State that the input will be a single combined text string derived from the row \"\n",
        "            f\"(including '{TEXT_COL}' and any additional context the user has chosen to include).\"\n",
        "        )\n",
        "\n",
        "    system_msg = (\n",
        "        \"You are an expert in qualitative content analysis and LLM prompt design. \"\n",
        "        \"You will now design a classification system prompt and a validation batch plan.\"\n",
        "    )\n",
        "\n",
        "    user_msg = f\"\"\"\n",
        "The user provided this description of their labeled dataset:\n",
        "\n",
        "{user_description_for_agent}\n",
        "\n",
        "Columns:\n",
        "- Text column: '{TEXT_COL}'\n",
        "- Label column: '{LABEL_COL}'\n",
        "\n",
        "{classifier_input_desc}\n",
        "\n",
        "The set of labels present in the data is:\n",
        "{labels}\n",
        "\n",
        "The full dataset has {total_examples} rows.\n",
        "Label frequencies in the full dataset are:\n",
        "{label_counts_text}\n",
        "\n",
        "Earlier, you requested a tool call to inspect the data:\n",
        "- Tool name: {tool_name}\n",
        "- Parameters: {json.dumps(tool_params)}\n",
        "\n",
        "Here is the JSON result of that tool call:\n",
        "{tool_result_str}\n",
        "\n",
        "Your tasks now:\n",
        "\n",
        "1. Draft a clear, concise **system prompt** (in English) for a classifier model that will be used on one row at a time.\n",
        "   The system prompt should:\n",
        "   - Instruct the classification task in general terms, using the user's description.\n",
        "   {input_requirement_sentence}\n",
        "   - State that the model must assign exactly ONE label from the list: {labels}.\n",
        "   - Require strict JSON output of the form:\n",
        "       {{\"label\": \"<one of: {', '.join(labels)}>\"}}\n",
        "   - Be independent of any specific row values (i.e., no references to particular examples by content).\n",
        "\n",
        "2. Propose how many examples of each label to include in the **first validation batch**.\n",
        "   - You MUST respect the true label frequencies above, not exceeding the label's maximum available count.\n",
        "   - Be mindful of cost: the total validation batch should be informative but not unnecessarily large.\n",
        "\n",
        "Return ONLY a JSON object of the form:\n",
        "\n",
        "{{\n",
        "  \"classification_system_prompt\": \"<English system prompt for the classifier>\",\n",
        "  \"per_label_batch_sizes\": {{\n",
        "    \"<label_1>\": <int>,\n",
        "    \"<label_2>\": <int>,\n",
        "    ...\n",
        "  }},\n",
        "  \"justification\": \"<brief English justification of your plan>\"\n",
        "}}\n",
        "\n",
        "Make sure that:\n",
        "- The keys in per_label_batch_sizes match the labels exactly: {labels}.\n",
        "- For every label L, per_label_batch_sizes[L] is an integer between 0 and label_counts[L], inclusive.\n",
        "\"\"\"\n",
        "\n",
        "    resp = openai_chat_via_cf(\n",
        "        model=MODEL_AGENT,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_msg},\n",
        "            {\"role\": \"user\", \"content\": user_msg},\n",
        "        ],\n",
        "        temperature=TEMPERATURE_AGENT,\n",
        "    )\n",
        "\n",
        "    content = resp[\"choices\"][0][\"message\"][\"content\"]\n",
        "    print(\"\\n=== Agent prompt + validation plan (raw) ===\")\n",
        "    print(content)\n",
        "\n",
        "    plan = json.loads(content)\n",
        "    return plan"
      ],
      "metadata": {
        "id": "pqQAJzk0YgM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###---Helper Function (validation batch building)---###\n",
        "\n",
        "#for the first validation round\n",
        "def build_validation_batch(df,\n",
        "                           label_col,\n",
        "                           labels_list,\n",
        "                           per_label_batch_sizes,\n",
        "                           random_state):\n",
        "    dfs = []\n",
        "    #for each candidate label in the df,\n",
        "    #randomly sample the number of rows the agent requests\n",
        "    for lab in labels_list:\n",
        "        #look up how many examples were requested for this label\n",
        "        requested = int(per_label_batch_sizes.get(lab, 0))\n",
        "        if requested <= 0:\n",
        "            continue  #defensive fallback\n",
        "        subset = df[df[label_col] == lab]\n",
        "        if len(subset) == 0:\n",
        "            continue  #defensive fallback\n",
        "        sample_n = min(requested, len(subset))\n",
        "        sampled = subset.sample(n=sample_n, random_state=random_state)\n",
        "        dfs.append(sampled)\n",
        "\n",
        "    if not dfs:\n",
        "        raise ValueError(\"No data selected for validation batch. Check agent plan.\")\n",
        "\n",
        "    #build the final validation batch dataframe\n",
        "    batch_df_local = (\n",
        "        #stack all sampled dataframes vertically into a single dataframe\n",
        "        pd.concat(dfs, axis=0)\n",
        "        #shuffle all rows in that concatenated dataframe\n",
        "        .sample(frac=1.0, random_state=random_state)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    return batch_df_local\n",
        "\n",
        "#more flexible validation batch sampling for 2nd+ validation rounds\n",
        "def build_validation_batch_generic(\n",
        "    df,\n",
        "    evaluated_ids,\n",
        "    label_col,\n",
        "    labels_list,\n",
        "    per_label_batch_sizes_val,\n",
        "    sample_from_val,\n",
        "    random_state,\n",
        "):\n",
        "    #only sampling from the remaining unused rows\n",
        "    if sample_from_val == \"remaining_only\":\n",
        "        pool_df = df[~df[\"row_id\"].isin(evaluated_ids)]\n",
        "    #allow resampling from the full df\n",
        "    else:\n",
        "        pool_df = df\n",
        "\n",
        "    if pool_df.empty:\n",
        "        raise ValueError(\"No rows available to sample from.\")\n",
        "\n",
        "    dfs_local = []\n",
        "    for lab in labels_list:\n",
        "        requested = int(per_label_batch_sizes_val.get(lab, 0))\n",
        "        if requested <= 0:\n",
        "            continue\n",
        "        subset = pool_df[pool_df[label_col] == lab]\n",
        "        if len(subset) == 0:\n",
        "            continue\n",
        "        sample_n = min(requested, len(subset))\n",
        "        sampled = subset.sample(n=sample_n, random_state=random_state)\n",
        "        dfs_local.append(sampled)\n",
        "\n",
        "    if not dfs_local:\n",
        "        raise ValueError(\"No data selected for new validation batch.\")\n",
        "    batch_df_local = (\n",
        "        pd.concat(dfs_local, axis=0)\n",
        "        .sample(frac=1.0, random_state=random_state)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    return batch_df_local"
      ],
      "metadata": {
        "id": "47krZLKqc_qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###---Helper Function (classification worker)---###\n",
        "\n",
        "def classify_one(\n",
        "    row,\n",
        "    system_prompt,\n",
        "    labels_list,\n",
        "    majority_label_val,\n",
        "    MODEL_WORKER,\n",
        "    TEXT_COL,\n",
        "    row_to_text_fn,\n",
        "    temperature=0,\n",
        "):\n",
        "\n",
        "    #construct the exact text that will be fed to the worker LLM\n",
        "    text_value = model_input_from_row(row, TEXT_COL, row_to_text_fn)\n",
        "    has_row_to_text_fn = row_to_text_fn is not None\n",
        "\n",
        "    if not has_row_to_text_fn:\n",
        "        input_source_line = f\"Input text (raw value from column '{TEXT_COL}'):\"\n",
        "    else:\n",
        "        input_source_line = (\n",
        "            \"Input text: a single combined string built from the row \"\n",
        "            f\"(including '{TEXT_COL}' and any other context columns the user chose).\"\n",
        "        )\n",
        "\n",
        "    #build a small instruction in the user message\n",
        "    user_msg = f\"\"\"\n",
        "You will be given a text (and possible additional contexts).\n",
        "\n",
        "Your task:\n",
        "- Based on the system instructions, decide which label best applies to this text.\n",
        "- You must choose exactly ONE label from this set:\n",
        "  {labels_list}\n",
        "\n",
        "{input_source_line}\n",
        "{text_value}\n",
        "\n",
        "Output format:\n",
        "Return ONLY a single JSON object with one field \"label\", whose value is exactly one of:\n",
        "{labels_list}\n",
        "\n",
        "For example:\n",
        "  {{\"label\": \"{labels_list[0]}\"}}   (if that is the best label)\n",
        "No extra text, no explanation.\n",
        "\"\"\"\n",
        "\n",
        "    #call LLM API\n",
        "    resp = openai_chat_via_cf(\n",
        "        model=MODEL_WORKER,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_msg},\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "    )\n",
        "\n",
        "    #extract the worker's output from the response\n",
        "    content = resp[\"choices\"][0][\"message\"][\"content\"]\n",
        "    try:\n",
        "        obj = json.loads(content)\n",
        "        label = obj.get(\"label\", \"\").strip()\n",
        "        if label not in labels_list:\n",
        "            raise ValueError(\"Invalid label\")\n",
        "    except Exception:\n",
        "        label = None\n",
        "        for lab in labels_list:\n",
        "            if lab in content:\n",
        "                label = lab\n",
        "                break\n",
        "        if label is None:\n",
        "            label = majority_label_val\n",
        "    return label\n",
        "\n",
        "#loop over a batch of rows, call classify_one for each\n",
        "def classify_batch(\n",
        "    df_batch,\n",
        "    system_prompt,\n",
        "    labels_list,\n",
        "    majority_label_val,\n",
        "    worker_config,\n",
        "    MODEL_WORKER,\n",
        "    TEXT_COL,\n",
        "    row_to_text_fn,\n",
        "):\n",
        "\n",
        "    temp = float(worker_config.get(\"temperature\", TEMPERATURE_WORKER))\n",
        "    preds = [\n",
        "        classify_one(\n",
        "            row,\n",
        "            system_prompt,\n",
        "            labels_list,\n",
        "            majority_label_val,\n",
        "            MODEL_WORKER,\n",
        "            TEXT_COL,\n",
        "            row_to_text_fn,\n",
        "            temperature=temp,\n",
        "        )\n",
        "        for _, row in df_batch.iterrows()\n",
        "    ]\n",
        "\n",
        "    #attach predictions as a new column, return the labeled batch\n",
        "    df_batch_local = df_batch.copy()\n",
        "    df_batch_local[\"pred_label\"] = preds\n",
        "    return df_batch_local\n"
      ],
      "metadata": {
        "id": "ruz8E5ZkhHfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###---Helper Function (Agent decides the next step)---###\n",
        "\n",
        "def agent_decide_next_step(\n",
        "    user_description_for_agent,\n",
        "    TEXT_COL,\n",
        "    LABEL_COL,\n",
        "    labels_list,\n",
        "    label_counts_val,\n",
        "    used_counts_val,\n",
        "    remaining_counts_val,\n",
        "    classification_system_prompt_val,\n",
        "    metrics_summary_val,\n",
        "    mis_examples_val,\n",
        "    total_examples,\n",
        "    num_evaluated_val,\n",
        "    MODEL_AGENT,\n",
        "    has_row_to_text_fn,\n",
        "):\n",
        "\n",
        "    #convert classification metrics and misclassified examples to strings\n",
        "    metrics_text = json.dumps(metrics_summary_val, ensure_ascii=False)\n",
        "    mis_text = json.dumps(mis_examples_val, ensure_ascii=False)\n",
        "\n",
        "    #basic summaries of the data\n",
        "    label_counts_text = \"\\n\".join(\n",
        "        f\"- {lab}: {count} total rows\" for lab, count in label_counts_val.items()\n",
        "    )\n",
        "    used_counts_text = \"\\n\".join(\n",
        "        f\"- {lab}: {used_counts_val.get(lab, 0)} already evaluated\" for lab in labels_list\n",
        "    )\n",
        "    remaining_counts_text = \"\\n\".join(\n",
        "        f\"- {lab}: {remaining_counts_val.get(lab, 0)} remaining\" for lab in labels_list\n",
        "    )\n",
        "\n",
        "    if not has_row_to_text_fn:\n",
        "        input_summary_for_agent = (\n",
        "            f\"The classifier currently receives as input only the value of '{TEXT_COL}'.\"\n",
        "        )\n",
        "    else:\n",
        "        input_summary_for_agent = (\n",
        "            \"The classifier currently receives as input a combined string built from each row \"\n",
        "            f\"(including '{TEXT_COL}' and possibly other columns that the user has chosen).\"\n",
        "        )\n",
        "\n",
        "    system_msg = (\n",
        "        \"You are an expert in qualitative content analysis and prompt design. \"\n",
        "        \"You are acting as an agent that can iteratively improve or accept a prompt.\"\n",
        "    )\n",
        "\n",
        "    user_msg = f\"\"\"\n",
        "The user provided this description of their labeled dataset:\n",
        "\n",
        "{user_description_for_agent}\n",
        "\n",
        "Columns:\n",
        "- Text column: '{TEXT_COL}'\n",
        "- Label column: '{LABEL_COL}'\n",
        "\n",
        "{input_summary_for_agent}\n",
        "\n",
        "Labels present in the data:\n",
        "{labels_list}\n",
        "\n",
        "Label frequencies in the full dataset:\n",
        "{label_counts_text}\n",
        "\n",
        "Evaluation status so far:\n",
        "- Total rows evaluated so far: {num_evaluated_val} out of {total_examples}\n",
        "- Per-label evaluated counts:\n",
        "{used_counts_text}\n",
        "- Per-label remaining counts:\n",
        "{remaining_counts_text}\n",
        "\n",
        "Current system prompt for the classifier:\n",
        "\n",
        "--- BEGIN CURRENT SYSTEM PROMPT ---\n",
        "{classification_system_prompt_val}\n",
        "--- END CURRENT SYSTEM PROMPT ---\n",
        "\n",
        "Evaluation summary over all evaluated rows (JSON):\n",
        "{metrics_text}\n",
        "\n",
        "Some misclassified examples (if any), truncated:\n",
        "{mis_text}\n",
        "\n",
        "You must now decide what to do next. You have three high-level options:\n",
        "\n",
        "1. accept_prompt\n",
        "   - Keep the current system prompt as the final prompt.\n",
        "   - Do not request further validation in this step.\n",
        "\n",
        "2. validate_more\n",
        "   - Keep the current system prompt.\n",
        "   - Request another validation round.\n",
        "   - You will specify:\n",
        "       (a) validation_mode:\n",
        "           - \"reuse_last_batch\": re-evaluate the SAME rows as the last validation batch.\n",
        "           - \"new_subset\": evaluate a NEW subset of rows.\n",
        "       (b) If validation_mode = \"new_subset\", also specify sample_from:\n",
        "           - \"remaining_only\": sample only from rows not yet evaluated.\n",
        "           - \"full_dataset\": sample from the entire dataset (you may re-use some rows).\n",
        "       (c) per_label_batch_sizes: for each label, how many rows to include in this next validation round.\n",
        "\n",
        "3. revise_prompt\n",
        "   - Propose an improved system prompt (in English).\n",
        "   - Also request another validation round, with the same controls as in (2):\n",
        "       validation_mode, sample_from, per_label_batch_sizes.\n",
        "\n",
        "Cost & sample-size considerations (important):\n",
        "- Each additional evaluated row increases cost roughly linearly.\n",
        "- Often, relatively small but well-chosen batches (for example, dozens of examples per label, not hundreds) are enough to see whether a prompt revision helped.\n",
        "- When performance is clearly low (e.g., low macro-F1), prioritize:\n",
        "    * understanding errors and revising the prompt, and\n",
        "    * testing revised prompts on modest, targeted batches,\n",
        "  instead of repeatedly requesting much larger batches.\n",
        "\n",
        "You may also optionally specify worker_config, such as temperature, for the classifier model.\n",
        "\n",
        "Return ONLY a JSON object of the form:\n",
        "\n",
        "{{\n",
        "  \"decision\": \"accept_prompt\" | \"validate_more\" | \"revise_prompt\",\n",
        "  \"validation_mode\": \"reuse_last_batch\" | \"new_subset\",\n",
        "  \"sample_from\": \"remaining_only\" | \"full_dataset\",\n",
        "  \"worker_config\": {{\n",
        "    \"temperature\": <float between 0.0 and 1.0, or omit to use default>\n",
        "  }},\n",
        "  \"new_system_prompt\": \"<string or null>\",\n",
        "  \"per_label_batch_sizes\": {{\n",
        "    \"<label_1>\": <int>,\n",
        "    \"<label_2>\": <int>,\n",
        "    ...\n",
        "  }},\n",
        "  \"justification\": \"<brief English justification>\"\n",
        "}}\n",
        "\n",
        "Conventions:\n",
        "- If you choose \"accept_prompt\", set \"validation_mode\" to \"reuse_last_batch\",\n",
        "  \"sample_from\" can be null or \"remaining_only\", worker_config can be empty, and\n",
        "  per_label_batch_sizes can be empty. The prompt will be considered final.\n",
        "- For \"validate_more\" or \"revise_prompt\":\n",
        "  - validation_mode and sample_from MUST be specified.\n",
        "  - Keys of per_label_batch_sizes must match the labels exactly: {labels_list}.\n",
        "  - If sample_from = \"remaining_only\", you should respect remaining_counts[label]\n",
        "    when choosing batch sizes.\n",
        "\"\"\"\n",
        "\n",
        "    resp = openai_chat_via_cf(\n",
        "        model=MODEL_AGENT,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_msg},\n",
        "            {\"role\": \"user\", \"content\": user_msg},\n",
        "        ],\n",
        "        temperature=TEMPERATURE_AGENT,\n",
        "    )\n",
        "\n",
        "    content = resp[\"choices\"][0][\"message\"][\"content\"]\n",
        "    print(\"\\n=== Agent next-step decision (raw) ===\")\n",
        "    print(content)\n",
        "\n",
        "    decision_plan_val = json.loads(content)\n",
        "    return decision_plan_val\n"
      ],
      "metadata": {
        "id": "EcnBgxrVVzCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Final Pipeline Wrap-Up Function"
      ],
      "metadata": {
        "id": "q21wqk2JLIJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###---Helper Function (final wrap-up)---###\n",
        "\n",
        "def run_agentic_labeling_pipeline(\n",
        "    df_train,\n",
        "    df_test,\n",
        "    TEXT_COL=\"text\",\n",
        "    LABEL_COL=\"human_label\",\n",
        "    MODEL_AGENT=\"gpt-5.1\",\n",
        "    MODEL_WORKER=\"gpt-5-mini\",\n",
        "    RANDOM_SEED=2025,\n",
        "    user_description_for_agent=\"\",\n",
        "    max_rounds=3,\n",
        "    row_to_text_fn=None,   #optionally, build input string from multiple columns\n",
        "):\n",
        "    random.seed(RANDOM_SEED)\n",
        "\n",
        "    # === Stage 0: prepare training data ===\n",
        "    #make a copy of the training df so the original object won't be accidentally modified\n",
        "    df_train = df_train.copy()\n",
        "    df_train = df_train.reset_index().rename(columns={\"index\": \"row_id\"})\n",
        "    #ensure labels are clean\n",
        "    df_train[LABEL_COL] = df_train[LABEL_COL].astype(str).str.strip()\n",
        "\n",
        "    labels = sorted(df_train[LABEL_COL].unique().tolist())\n",
        "    label_counts = df_train[LABEL_COL].value_counts().to_dict()\n",
        "    majority_label = max(label_counts.items(), key=lambda kv: kv[1])[0]\n",
        "\n",
        "    print(\"=== Stage 0: Prepare training data ===\")\n",
        "    print(f\"  Labels found: {labels}\")\n",
        "    print(f\"  Label counts: {label_counts}\")\n",
        "    print(f\"  Total train rows: {len(df_train)}\\n\")\n",
        "\n",
        "    has_row_to_text_fn = row_to_text_fn is not None\n",
        "\n",
        "    # === Stage 1: agent chooses data view ===\n",
        "    print(\"=== Stage 1: Agent chooses data view on training set ===\")\n",
        "    view_cmd = agent_choose_data_view(\n",
        "        user_description_for_agent,\n",
        "        TEXT_COL,\n",
        "        LABEL_COL,\n",
        "        labels,\n",
        "        label_counts,\n",
        "        MODEL_AGENT,\n",
        "    )\n",
        "\n",
        "    tool_name, tool_params, tool_result = execute_data_view_command(\n",
        "        df_train,\n",
        "        TEXT_COL,\n",
        "        LABEL_COL,\n",
        "        labels,\n",
        "        label_counts,\n",
        "        view_cmd,\n",
        "        RANDOM_SEED,\n",
        "        row_to_text_fn=row_to_text_fn,\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Tool executed ===\")\n",
        "    print(\"  Tool:\", tool_name)\n",
        "    print(\"  Params:\", tool_params)\n",
        "\n",
        "    if tool_name == \"python_view\":\n",
        "        print(\"  Status:\", tool_result.get(\"status\"))\n",
        "        stdout_full = tool_result.get(\"stdout\") or \"\"\n",
        "        stderr_full = tool_result.get(\"stderr\") or \"\"\n",
        "        stdout_preview = stdout_full[:1500]\n",
        "        stderr_preview = stderr_full[:1500]\n",
        "\n",
        "        if stdout_preview:\n",
        "            print(\"  Stdout preview:\\n\", stdout_preview + (\"...\" if len(stdout_full) > 1500 else \"\"))\n",
        "        if stderr_preview.strip():\n",
        "            print(\"\\n  Stderr preview:\\n\", stderr_preview + (\"...\" if len(stderr_full) > 1500 else \"\"))\n",
        "    else:\n",
        "        preview_str = json.dumps(tool_result, ensure_ascii=False)[:1500]\n",
        "        full_str = json.dumps(tool_result, ensure_ascii=False)\n",
        "        print(\"  Result preview:\", preview_str + (\"...\" if len(full_str) > 1500 else \"\"))\n",
        "\n",
        "    # === Stage 2: agent builds initial prompt + plan ===\n",
        "    print(\"\\n=== Stage 2: Agent builds initial prompt + validation plan ===\")\n",
        "    plan = agent_build_prompt_and_plan(\n",
        "        user_description_for_agent=user_description_for_agent,\n",
        "        TEXT_COL=TEXT_COL,\n",
        "        LABEL_COL=LABEL_COL,\n",
        "        labels=labels,\n",
        "        label_counts=label_counts,\n",
        "        tool_name=tool_name,\n",
        "        tool_params=tool_params,\n",
        "        tool_result=tool_result,\n",
        "        total_examples=len(df_train),\n",
        "        MODEL_AGENT=MODEL_AGENT,\n",
        "        has_row_to_text_fn=has_row_to_text_fn,\n",
        "    )\n",
        "\n",
        "    classification_system_prompt = plan[\"classification_system_prompt\"]\n",
        "    per_label_batch_sizes = plan[\"per_label_batch_sizes\"]\n",
        "\n",
        "    print(\"\\n=== Agent-chosen per_label_batch_sizes ===\")\n",
        "    print(per_label_batch_sizes)\n",
        "    print(\"\\n=== Initial classification system prompt ===\\n\")\n",
        "    print(classification_system_prompt)\n",
        "\n",
        "    # === Stage 3: build initial validation batch ===\n",
        "    print(\"\\n=== Stage 3: Build initial validation batch ===\")\n",
        "    for lab in labels:\n",
        "        requested = int(per_label_batch_sizes.get(lab, 0))\n",
        "        available = label_counts.get(lab, 0)\n",
        "        if requested > available:\n",
        "            print(\n",
        "                f\"  Warning: agent requested {requested} rows for label '{lab}' \"\n",
        "                f\"but only {available} exist; capping at {available}.\"\n",
        "            )\n",
        "\n",
        "    batch_df = build_validation_batch(\n",
        "        df_train,\n",
        "        label_col=LABEL_COL,\n",
        "        labels_list=labels,\n",
        "        per_label_batch_sizes=per_label_batch_sizes,\n",
        "        random_state=RANDOM_SEED,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"  Validation batch size: {len(batch_df)} \"\n",
        "        + \", \".join(f\"{lab}: {(batch_df[LABEL_COL] == lab).sum()}\" for lab in labels)\n",
        "    )\n",
        "\n",
        "    # === Stage 4: first validation round ===\n",
        "    print(\"\\n=== Stage 4: First validation round ===\")\n",
        "    worker_config = {}\n",
        "    batch_df = classify_batch(\n",
        "        batch_df,\n",
        "        classification_system_prompt,\n",
        "        labels,\n",
        "        majority_label,\n",
        "        worker_config,\n",
        "        MODEL_WORKER,\n",
        "        TEXT_COL,\n",
        "        row_to_text_fn,\n",
        "    )\n",
        "\n",
        "    eval_df = batch_df.copy()\n",
        "    evaluated_ids = set(eval_df[\"row_id\"].tolist())\n",
        "    num_evaluated_so_far = len(eval_df)\n",
        "\n",
        "    metrics_summary = build_metrics_summary(eval_df, LABEL_COL, \"pred_label\", labels)\n",
        "    used_counts = eval_df[LABEL_COL].value_counts().to_dict()\n",
        "    remaining_counts = {\n",
        "        lab: label_counts.get(lab, 0) - used_counts.get(lab, 0) for lab in labels\n",
        "    }\n",
        "\n",
        "    mis_all = eval_df[eval_df[LABEL_COL] != eval_df[\"pred_label\"]]\n",
        "    mis_examples_for_agent = [\n",
        "        {\n",
        "            \"text\": row[TEXT_COL],\n",
        "            \"true_label\": row[LABEL_COL],\n",
        "            \"pred_label\": row[\"pred_label\"],\n",
        "        }\n",
        "        for _, row in mis_all.head(10).iterrows()\n",
        "    ]\n",
        "\n",
        "    print(\"\\n--- Classification report on initial validation batch ---\")\n",
        "    print(\n",
        "        classification_report(\n",
        "            eval_df[LABEL_COL],\n",
        "            eval_df[\"pred_label\"],\n",
        "            labels=labels,\n",
        "            digits=3,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # === Stage 5: iterative agent loop ===\n",
        "    print(\"\\n=== Stage 5: Iterative agent loop ===\")\n",
        "    final_prompt = None\n",
        "    last_batch_df = batch_df\n",
        "\n",
        "    for round_idx in range(1, max_rounds + 1):\n",
        "        print(f\"\\n----- Agent iteration {round_idx} -----\")\n",
        "        decision_plan = agent_decide_next_step(\n",
        "            user_description_for_agent=user_description_for_agent,\n",
        "            TEXT_COL=TEXT_COL,\n",
        "            LABEL_COL=LABEL_COL,\n",
        "            labels_list=labels,\n",
        "            label_counts_val=label_counts,\n",
        "            used_counts_val=used_counts,\n",
        "            remaining_counts_val=remaining_counts,\n",
        "            classification_system_prompt_val=classification_system_prompt,\n",
        "            metrics_summary_val=metrics_summary,\n",
        "            mis_examples_val=mis_examples_for_agent,\n",
        "            total_examples=len(df_train),\n",
        "            num_evaluated_val=num_evaluated_so_far,\n",
        "            MODEL_AGENT=MODEL_AGENT,\n",
        "            has_row_to_text_fn=has_row_to_text_fn,\n",
        "        )\n",
        "\n",
        "        print(\"\\n=== Parsed agent decision plan ===\")\n",
        "        print(decision_plan)\n",
        "\n",
        "        decision = decision_plan[\"decision\"]\n",
        "        validation_mode = decision_plan.get(\"validation_mode\", \"new_subset\")\n",
        "        sample_from = decision_plan.get(\"sample_from\", \"remaining_only\")\n",
        "        worker_config = decision_plan.get(\"worker_config\", {}) or {}\n",
        "        new_system_prompt = decision_plan.get(\"new_system_prompt\")\n",
        "        next_per_label_batch_sizes = decision_plan.get(\"per_label_batch_sizes\", {}) or {}\n",
        "\n",
        "        if decision == \"accept_prompt\":\n",
        "            final_prompt = classification_system_prompt\n",
        "            print(\"\\nAgent chose to ACCEPT the current prompt as final.\")\n",
        "            break\n",
        "\n",
        "        if decision == \"revise_prompt\" and new_system_prompt:\n",
        "            classification_system_prompt = new_system_prompt\n",
        "            print(\"\\nAgent chose to REVISE the prompt.\")\n",
        "            print(\"Revised system prompt:\\n\", classification_system_prompt)\n",
        "        else:\n",
        "            print(\"\\nAgent chose to VALIDATE MORE with the same prompt.\")\n",
        "\n",
        "        print(\"  validation_mode:\", validation_mode)\n",
        "        print(\"  sample_from:\", sample_from)\n",
        "        print(\"  worker_config:\", worker_config)\n",
        "        print(\"  Next per_label_batch_sizes:\", next_per_label_batch_sizes)\n",
        "\n",
        "        if validation_mode == \"reuse_last_batch\":\n",
        "            new_last_batch = classify_batch(\n",
        "                last_batch_df,\n",
        "                classification_system_prompt,\n",
        "                labels,\n",
        "                majority_label,\n",
        "                worker_config,\n",
        "                MODEL_WORKER,\n",
        "                TEXT_COL,\n",
        "                row_to_text_fn,\n",
        "            )\n",
        "            last_batch_ids = last_batch_df[\"row_id\"].tolist()\n",
        "            eval_df = eval_df.copy()\n",
        "            mask = eval_df[\"row_id\"].isin(last_batch_ids)\n",
        "            eval_df.loc[mask, \"pred_label\"] = new_last_batch[\"pred_label\"].values\n",
        "            last_batch_df = new_last_batch\n",
        "        else:\n",
        "            next_batch_df = build_validation_batch_generic(\n",
        "                df_train,\n",
        "                evaluated_ids=evaluated_ids,\n",
        "                label_col=LABEL_COL,\n",
        "                labels_list=labels,\n",
        "                per_label_batch_sizes_val=next_per_label_batch_sizes,\n",
        "                sample_from_val=sample_from,\n",
        "                random_state=RANDOM_SEED + round_idx,\n",
        "            )\n",
        "            next_batch_df = classify_batch(\n",
        "                next_batch_df,\n",
        "                classification_system_prompt,\n",
        "                labels,\n",
        "                majority_label,\n",
        "                worker_config,\n",
        "                MODEL_WORKER,\n",
        "                TEXT_COL,\n",
        "                row_to_text_fn,\n",
        "            )\n",
        "\n",
        "            eval_df = pd.concat([eval_df, next_batch_df], ignore_index=True)\n",
        "            evaluated_ids.update(next_batch_df[\"row_id\"].tolist())\n",
        "            last_batch_df = next_batch_df\n",
        "\n",
        "        num_evaluated_so_far = len(eval_df)\n",
        "        metrics_summary = build_metrics_summary(eval_df, LABEL_COL, \"pred_label\", labels)\n",
        "        used_counts = eval_df[LABEL_COL].value_counts().to_dict()\n",
        "        remaining_counts = {\n",
        "            lab: label_counts.get(lab, 0) - used_counts.get(lab, 0) for lab in labels\n",
        "        }\n",
        "\n",
        "        mis_all = eval_df[eval_df[LABEL_COL] != eval_df[\"pred_label\"]]\n",
        "        mis_examples_for_agent = [\n",
        "            {\n",
        "                \"text\": row[TEXT_COL],\n",
        "                \"true_label\": row[LABEL_COL],\n",
        "                \"pred_label\": row[\"pred_label\"],\n",
        "            }\n",
        "            for _, row in mis_all.head(10).iterrows()\n",
        "        ]\n",
        "\n",
        "        print(\"\\n--- Updated classification report on all evaluated rows ---\")\n",
        "        print(\n",
        "            classification_report(\n",
        "                eval_df[LABEL_COL],\n",
        "                eval_df[\"pred_label\"],\n",
        "                labels=labels,\n",
        "                digits=3,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    if final_prompt is None:\n",
        "        final_prompt = classification_system_prompt\n",
        "        print(\"\\nMax rounds reached; using last prompt as final.\")\n",
        "\n",
        "    # === Stage 6: final evaluation on TEST set ===\n",
        "    print(\"\\n=== Stage 6: Final evaluation on TEST set ===\")\n",
        "    df_test = df_test.copy()\n",
        "    df_test[LABEL_COL] = df_test[LABEL_COL].astype(str).str.strip()\n",
        "\n",
        "    print(f\"  Test size: {len(df_test)}\")\n",
        "    print(\"  Test label counts:\")\n",
        "    print(df_test[LABEL_COL].value_counts())\n",
        "\n",
        "    test_preds = [\n",
        "        classify_one(\n",
        "            row,\n",
        "            final_prompt,\n",
        "            labels,\n",
        "            majority_label,\n",
        "            MODEL_WORKER,\n",
        "            TEXT_COL,\n",
        "            row_to_text_fn,\n",
        "        )\n",
        "        for _, row in df_test.iterrows()\n",
        "    ]\n",
        "    df_test[\"pred_label\"] = test_preds\n",
        "\n",
        "    print(\"\\n--- Classification report on TEST set ---\")\n",
        "    print(\n",
        "        classification_report(\n",
        "            df_test[LABEL_COL],\n",
        "            df_test[\"pred_label\"],\n",
        "            labels=labels,\n",
        "            digits=3,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    mis_test = df_test[df_test[LABEL_COL] != df_test[\"pred_label\"]]\n",
        "    print(\"\\nSome misclassified TEST examples:\")\n",
        "    for _, row in mis_test.head(10).iterrows():\n",
        "        print(\"----\")\n",
        "        print(f\"Text from '{TEXT_COL}':\", row[TEXT_COL])\n",
        "        print(\"Gold:\", row[LABEL_COL], \" Pred:\", row[\"pred_label\"])\n",
        "\n",
        "    metrics_test = build_metrics_summary(df_test, LABEL_COL, \"pred_label\", labels)\n",
        "\n",
        "    return {\n",
        "        \"final_prompt\": final_prompt,\n",
        "        \"train_eval_df\": eval_df,\n",
        "        \"test_df\": df_test,\n",
        "        \"train_metrics\": metrics_summary,\n",
        "        \"test_metrics\": metrics_test,\n",
        "    }\n"
      ],
      "metadata": {
        "id": "oOMs1zcmV0XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Experiment 1: Chinese News Articles (Challenge vs. Source)\n",
        "In this experiment, the two-agent pipeline is applied to classify whether a Chinese news article excerpt, which mentions a foreign news outlet, is **challenging** (i.e., questioning the credibility, criticizing) the foreign news outlet, or citing it as an authoritative **source**.\n",
        "\n",
        "Data Source:\n",
        "Waight, Hannah et al. (Mar. 18, 2025). “The decade-long growth of government-authored news media in China.” In: Proceedings of the National Academy of Sciences 122.11. Publisher: Proceedings of the National Academy of Sciences, e2408260122. DOI: 10.1073/pnas.2408260122. URL: https://www.pnas.org/doi/10.1073/pnas.2408260122.\n",
        "\n",
        "Due to legal restrictions, this dataset is not made public."
      ],
      "metadata": {
        "id": "27kfBW_LLQZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###---User Setting---###\n",
        "CSV_PATH_train = data_path + \"data/human_labeled_sample.csv\"\n",
        "CSV_PATH_test  = data_path + \"data/human_labeled_validation_sample.csv\"\n",
        "\n",
        "df_train = pd.read_csv(CSV_PATH_train)\n",
        "df_test  = pd.read_csv(CSV_PATH_test)\n",
        "\n",
        "TEXT_COL = \"text\"          # default\n",
        "LABEL_COL = \"human_label\"  # default\n",
        "MODEL_AGENT = \"gpt-5.1\"\n",
        "MODEL_WORKER = \"gpt-5-mini\"\n",
        "RANDOM_SEED = 2025\n",
        "TEMPERATURE_AGENT = 0.3\n",
        "TEMPERATURE_WORKER = 0\n",
        "\n",
        "user_description_for_agent = \"\"\"This is a dataset of Chinese news article excerpts.\n",
        "There are two labels:\n",
        "Challenge: This Chinese news article excerpt explicitly challenges, criticizes, or disputes the foreign newspaper it mentions.\n",
        "Source: The editor mainly uses the foreign newspaper as a source of information or reporting.\"\"\"\n"
      ],
      "metadata": {
        "id": "jxr_E6H3V4Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = run_agentic_labeling_pipeline(\n",
        "    df_train=df_train,\n",
        "    df_test=df_test,\n",
        "    TEXT_COL=TEXT_COL,\n",
        "    LABEL_COL=LABEL_COL,\n",
        "    MODEL_AGENT=MODEL_AGENT,\n",
        "    MODEL_WORKER=MODEL_WORKER,\n",
        "    RANDOM_SEED=RANDOM_SEED,\n",
        "    user_description_for_agent=user_description_for_agent,\n",
        "    max_rounds=3,\n",
        "    row_to_text_fn=None\n",
        ")"
      ],
      "metadata": {
        "id": "O0-RK24HWTpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Experiment 2: EZ-STANCE Stance Detection\n",
        "\n",
        "The two agents perform the task of labeling the **stance** expressed in English tweets toward a given target. The goal is to determine the author's position or attitude (i.e., whether they *favor*, *oppose*, or remain *neutral* toward the subject mentioned).\n",
        "\n",
        "Open-Source Data:\n",
        "Chenye Zhao and Cornelia Caragea. 2024. EZ-STANCE: A Large Dataset for English Zero-Shot Stance Detection. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15697-15714, Bangkok, Thailand. Association for Computational Linguistics.\n"
      ],
      "metadata": {
        "id": "pmAdoE8xpxNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EZ_TRAIN_PATH = data_path + \"EZ-STANCE data/ezstance/subtaskA/mixed/raw_train_all_onecol.csv\"\n",
        "EZ_TEST_PATH  = data_path + \"EZ-STANCE data/ezstance/subtaskA/mixed/raw_test_all_onecol.csv\""
      ],
      "metadata": {
        "id": "hpomP4f5pwiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load raw EZ-STANCE files\n",
        "df_ezstance_train = pd.read_csv(EZ_TRAIN_PATH)\n",
        "df_ezstance_test = pd.read_csv(EZ_TEST_PATH)\n",
        "\n",
        "print(\"Train columns:\", df_ezstance_train.columns.tolist())\n",
        "print(\"Test columns :\", df_ezstance_test.columns.tolist())\n",
        "df_ezstance_train.head()\n",
        "\n",
        "TEST_N = 500\n",
        "df_ezstance_test = (\n",
        "    df_ezstance_test\n",
        "    .sample(\n",
        "        n=min(TEST_N, len(df_ezstance_test)),\n",
        "        random_state=RANDOM_SEED\n",
        "    )\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "print(\"Subsampled test size:\", len(df_ezstance_test))"
      ],
      "metadata": {
        "id": "u5GOZb2Ep1fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combining the target and post columns as the input\n",
        "def ezstance_row_to_text(row):\n",
        "    return f\"Target: {row['Target 1']}\\nPost: {row['Ori Text']}\""
      ],
      "metadata": {
        "id": "58COl5w0p48W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_ez = run_agentic_labeling_pipeline(\n",
        "    df_train=df_ezstance_train,\n",
        "    df_test=df_ezstance_test,\n",
        "    TEXT_COL=\"Ori Text\",\n",
        "    LABEL_COL=\"Stance 1\",\n",
        "    MODEL_AGENT=\"gpt-5.1\",\n",
        "    MODEL_WORKER=\"gpt-5-mini\",\n",
        "    RANDOM_SEED=2025,\n",
        "    user_description_for_agent=\"\"\"\n",
        "This is a dataset of English social media posts with stance labels toward a target.\n",
        "Labels: favor, against, none.\n",
        "\"\"\",\n",
        "    max_rounds=3,\n",
        "    row_to_text_fn=ezstance_row_to_text\n",
        ")"
      ],
      "metadata": {
        "id": "Z4L7D9XhqB9J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}